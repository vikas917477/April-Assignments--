{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b5af12",
   "metadata": {},
   "source": [
    "# Na√Øve bayes-2\n",
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e70be79",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ce199",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem:\n",
    "\n",
    "P(Smoker | Health Plan) = P(Health Plan | Smoker) * P(Smoker) / P(Health Plan)\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(Health Plan) = 0.7 (70% of employees use the health insurance plan)\n",
    "P(Smoker | Health Plan) = 0.4 (40% of employees who use the plan are smokers)\n",
    "P(Smoker) = unknown\n",
    "\n",
    "To find P(Smoker), we can use the law of total probability:\n",
    "\n",
    "P(Smoker) = P(Smoker | Health Plan) * P(Health Plan) + P(Smoker | No Health Plan) * P(No Health Plan)\n",
    "\n",
    "We are not given P(Smoker | No Health Plan), but we can assume that it is lower than P(Smoker | Health Plan), since smokers may be more likely to use the health insurance plan. For simplicity, let's assume that P(Smoker | No Health Plan) = 0.2 (20% of employees who don't use the plan are smokers), and P(No Health Plan) = 0.3 (30% of employees don't use the plan).\n",
    "\n",
    "Then, we can calculate:\n",
    "\n",
    "P(Smoker) = 0.4 * 0.7 + 0.2 * 0.3 = 0.32\n",
    "\n",
    "Now we can substitute these values into Bayes' theorem:\n",
    "\n",
    "P(Smoker | Health Plan) = 0.4 * P(Smoker) / P(Health Plan)\n",
    "P(Smoker | Health Plan) = 0.4 * 0.32 / 0.7\n",
    "P(Smoker | Health Plan) = 0.1829\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.1829 or 18.29%."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f840567b",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a6059",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is the type of data that they are designed to handle.\n",
    "\n",
    "Bernoulli Naive Bayes is typically used for binary data, where each feature can take on only two values (e.g., presence or absence of a certain word in a document). It models the likelihood of each feature given each class as a Bernoulli distribution, which represents the probability of a binary outcome (e.g., success or failure). Bernoulli Naive Bayes assumes that the features are conditionally independent given the class.\n",
    "\n",
    "Multinomial Naive Bayes, on the other hand, is designed for data that has multiple discrete values (e.g., word counts in a document). It models the likelihood of each feature given each class as a multinomial distribution, which represents the probability of observing a particular count for each possible value of the feature. Multinomial Naive Bayes assumes that the features are conditionally independent given the class.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used for binary data, while Multinomial Naive Bayes is used for count data. However, there may be cases where either approach can be used, depending on the specific problem and the nature of the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "abc81dc8",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cbb62",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values in a straightforward way: it simply ignores any features with missing values when making predictions. This is because Bernoulli Naive Bayes assumes that the features are conditionally independent given the class, so the absence of a feature is considered to be informative (i.e., it indicates that the feature is not present).\n",
    "\n",
    "For example, suppose we have a dataset with three binary features: X1, X2, and X3, and a binary class variable Y. If there is a missing value in X2 for a particular instance, we can simply omit X2 for that instance when calculating the likelihoods of each class. That is, we only consider the likelihoods of Y given X1 and X3.\n",
    "\n",
    "It is important to note that the effectiveness of this approach depends on the reason for the missing values. If the missing values are missing at random (i.e., the probability of missingness is independent of the true value of the feature), then this approach is valid. However, if the missing values are related to the class variable or other features in a systematic way, then ignoring them can lead to biased or inaccurate predictions. In such cases, more sophisticated imputation methods may be needed to handle missing values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1baffba3",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f729f",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. In this case, the model assumes that the distribution of each feature for each class is Gaussian (i.e., normal) and the features are independent given the class.\n",
    "\n",
    "To perform multi-class classification with Gaussian Naive Bayes, the model estimates the mean and variance of each feature for each class, and uses Bayes' theorem to calculate the posterior probability of each class given the observed values of the features. The class with the highest posterior probability is then selected as the predicted class.\n",
    "\n",
    "There are different methods for implementing multi-class classification with Gaussian Naive Bayes, such as one-vs-all (also called one-vs-rest) and one-vs-one. In the one-vs-all approach, the model trains a separate binary classifier for each class, where each classifier distinguishes between one class and the rest of the classes. In the one-vs-one approach, the model trains a binary classifier for each pair of classes, where each classifier distinguishes between two specific classes.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be used for multi-class classification, but the implementation may differ depending on the specific problem and the chosen method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "349837ea",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bedbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('spambase.data', header=None)\n",
    "\n",
    "# The last column contains the target variable (spam or not spam)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b73b4f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['accuracy', 'precision', 'recall', 'f1'] was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Perform 10-fold cross-validation and compute the performance metrics for each classifier\u001b[39;00m\n\u001b[0;32m     19\u001b[0m scoring \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m bernoulli_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbernoulli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m multinomial_scores \u001b[38;5;241m=\u001b[39m cross_val_score(multinomial, X, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m     22\u001b[0m gaussian_scores \u001b[38;5;241m=\u001b[39m cross_val_score(gaussian, X, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mscoring)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:507\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"Evaluate a score by cross-validation.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03m    loss function.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m scorer \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    510\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:482\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    478\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method. The estimator \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator\n\u001b[0;32m    480\u001b[0m         )\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scoring, Iterable):\n\u001b[1;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor evaluating multiple scores, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.model_selection.cross_validate instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m was passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scoring)\n\u001b[0;32m    486\u001b[0m     )\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscoring value should either be a callable, string or None. \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m was passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;241m%\u001b[39m scoring\n\u001b[0;32m    491\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: For evaluating multiple scores, use sklearn.model_selection.cross_validate instead. ['accuracy', 'precision', 'recall', 'f1'] was passed."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Instantiate the classifiers\n",
    "bernoulli = BernoulliNB()\n",
    "multinomial = MultinomialNB()\n",
    "gaussian = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and compute the performance metrics for each classifier\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "bernoulli_scores = cross_val_score(bernoulli, X, y, cv=10, scoring=scoring)\n",
    "multinomial_scores = cross_val_score(multinomial, X, y, cv=10, scoring=scoring)\n",
    "gaussian_scores = cross_val_score(gaussian, X, y, cv=10, scoring=scoring)\n",
    "\n",
    "# Print the performance metrics for each classifier\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy: \", np.mean(bernoulli_scores[:, 0]))\n",
    "print(\"Precision: \", np.mean(bernoulli_scores[:, 1]))\n",
    "print(\"Recall: \", np.mean(bernoulli_scores[:, 2]))\n",
    "print(\"F1 score: \", np.mean(bernoulli_scores[:, 3]))\n",
    "print()\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy: \", np.mean(multinomial_scores[:, 0]))\n",
    "print(\"Precision: \", np.mean(multinomial_scores[:, 1]))\n",
    "print(\"Recall: \", np.mean(multinomial_scores[:, 2]))\n",
    "print(\"F1 score: \", np.mean(multinomial_scores[:, 3]))\n",
    "print()\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy: \", np.mean(gaussian_scores[:, 0]))\n",
    "print(\"Precision: \", np.mean(gaussian_scores[:, 1]))\n",
    "print(\"Recall: \", np.mean(gaussian_scores[:, 2]))\n",
    "print(\"F1 score: \", np.mean(gaussian_scores[:, 3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc293e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
