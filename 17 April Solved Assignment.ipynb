{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30670d2d",
   "metadata": {},
   "source": [
    "# Boosting-2\n",
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a837a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90432bf1",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda4cbd",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression (GBR) is a machine learning algorithm that is used for regression problems, which involves predicting a continuous numerical value rather than a categorical value. GBR is a type of boosting algorithm that works by iteratively adding weak regression models to the ensemble, where each subsequent model is trained to correct the errors made by the previous models.\n",
    "\n",
    "At a high level, the GBR algorithm works as follows:\n",
    "\n",
    "1. A single regression model is trained on the training data.\n",
    "2. The residuals (i.e., the difference between the predicted and actual values) of the first model are calculated for each training sample.\n",
    "3. A second regression model is trained on the residuals, with the goal of predicting the residuals more accurately.\n",
    "4. The predictions of the second model are added to the predictions of the first model, producing a new set of predicted values.\n",
    "5. The residuals of the new set of predictions are calculated, and the process is repeated by training a third regression model on these residuals.\n",
    "6. The predictions of the third model are added to the predictions of the previous models, and the residuals of the new set of predictions are calculated.\n",
    "7. This process is repeated until a stopping criterion is met (e.g., a maximum number of iterations is reached, or the performance on the validation set does not improve).\n",
    "\n",
    "At each iteration, the new regression model is trained to minimize the loss function, which is typically the mean squared error (MSE) between the predicted and actual values. The loss function measures how well the current set of models fits the training data and is used to guide the training process.\n",
    "\n",
    "The final ensemble of regression models is obtained by summing the predictions of all the individual models in the ensemble. The weights of the individual models in the ensemble are determined by their performance on the training data, with better-performing models given higher weights. The final prediction of the GBR algorithm is the sum of the predictions of all the models in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde241b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "919ab83b",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee55dfb",
   "metadata": {},
   "source": [
    "- here's an implementation of gradient boosting regression from scratch using Python and NumPy. We'll use the Boston Housing dataset from scikit-learn as our example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5512f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 535.99\n",
      "R-squared: -6.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the gradient boosting regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residual as the difference between the true y and the mean y\n",
    "        residual = y - np.mean(y)\n",
    "\n",
    "        # Iterate over the number of estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # Fit a decision tree to the residual\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "\n",
    "            # Compute the predictions of the tree and update the residual\n",
    "            predictions = tree.predict(X)\n",
    "            residual -= self.learning_rate * predictions\n",
    "\n",
    "            # Add the tree to the list of estimators\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute the sum of the predictions of all the trees\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.estimators:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Train the gradient boosting regression model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean squared error: %.2f\" % mse)\n",
    "print(\"R-squared: %.2f\" % r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d64c4b",
   "metadata": {},
   "source": [
    "In this implementation, we define a GradientBoostingRegressor class that contains the hyperparameters for the algorithm (n_estimators, learning_rate, and max_depth) and the list of decision trees (estimators). The fit method trains the model by iterating over the number of estimators and fitting a decision tree to the residual of the previous prediction. The predict method computes the sum of the predictions of all the trees in the ensemble. Finally, we train the model on the Boston Housing dataset, evaluate it on the test data using mean squared error and R-squared, and print the evaluation metrics.\n",
    "\n",
    "Note that this implementation is a simplified version of gradient boosting regression, and there are many ways to improve it (e.g., by adding early stopping, regularization, or using different types of weak learners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbe14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9035851",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a69c0",
   "metadata": {},
   "source": [
    "- how to use grid search to find the best hyperparameters for the gradient boosting regression model we implemented earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19ba37",
   "metadata": {},
   "source": [
    "In this example, we define a parameter grid with different values for the n_estimators, learning_rate, and max_depth hyperparameters. We then create a GridSearchCV object with the GradientBoostingRegressor model and the parameter grid, and fit it to the training data using 5-fold cross-validation. Finally, we print the best hyperparameters and evaluate the best model on the test data using mean squared error and R-squared.\n",
    "\n",
    "You can also use random search instead of grid search to explore the hyperparameter space more efficiently. Here's an example of how to use random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44367883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b6cb9e",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c3f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
